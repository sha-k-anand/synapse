%%sql
CREATE TABLE zzDelta_Loan1
USING DELTA OPTIONS (path 'abfss://fs1@lendingclubadls001.dfs.core.windows.net/delta/t1/')



df = spark.read.load('abfss://tpch-sf1000-consolidated@tpchdata.dfs.core.windows.net/SUPPLIER/', format='parquet')
display(df.limit(10))


df.write.format("delta").partitionBy("S_NATIONKEY").save("abfss://zztest@tpchdata.dfs.core.windows.net/Delta_SUPPLIER_temp100/")

df.write.format("delta").partitionBy("S_NATIONKEY","S_NATIONKEY2").save("abfss://zztest@tpchdata.dfs.core.windows.net/Delta_SUPPLIER_temp100/")

%%sql
CREATE TABLE zzDelta_SUPPLIER_temp100
(
S_SUPPKEY bigint, 
S_NAME string,
S_ADDRESS string,
S_NATIONKEY bigint,
S_PHONE string,
S_ACCTBAL decimal(9,2) ,
S_COMMENT string
)
USING DELTA  PARTITIONED BY (S_NATIONKEY) OPTIONS (path 'abfss://zztest@tpchdata.dfs.core.windows.net/Delta_SUPPLIER_temp100/')

%%sql
select count(*) from zzDelta_SUPPLIER_temp100

%%sql
UPDATE zzDelta_SUPPLIER_temp100 SET S_COMMENT = 'test1112' WHERE S_NATIONKEY = 23



spark.sql("SET spark.databricks.delta.retentionDurationCheck.enabled = false")
spark.sql("VACUUM `zzDelta_SUPPLIER_part1` RETAIN 0 HOURS").show()


-- serverless SQL
SELECT
     result.filepath(1) as partfolder,
	 COUNT(DISTINCT result.filename()) as FileCount,
	 count(*) as RowCount1
FROM
    OPENROWSET(
        BULK 'https://tpchdata.dfs.core.windows.net/tpch-sf1000-synapse-deltalake/SUPPLIER2/*/*',
        FORMAT = 'PARQUET'
    ) AS [result]
GROUP BY result.filepath(1)
ORDER BY 1
