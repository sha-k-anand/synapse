df = spark.read.load('abfss://tpch-sf1000-consolidated@tpchdata.dfs.core.windows.net/SUPPLIER/', format='parquet')
display(df.limit(10))


df.write.format("delta").partitionBy("S_NATIONKEY").save("abfss://zztest@tpchdata.dfs.core.windows.net/Delta_SUPPLIER_part2/")

%%sql
CREATE TABLE zzDelta_SUPPLIER_part1 
(
S_SUPPKEY bigint, 
S_NAME string,
S_ADDRESS string,
S_NATIONKEY bigint,
S_PHONE string,
S_ACCTBAL decimal(9,2) ,
S_COMMENT string
)
USING DELTA  PARTITIONED BY (S_NATIONKEY) OPTIONS (path 'abfss://zztest@tpchdata.dfs.core.windows.net/Delta_SUPPLIER_part2/')


%%sql
select count(*) from zzDelta_SUPPLIER_part1

%%sql
UPDATE zzDelta_SUPPLIER_part1 SET S_COMMENT = 'test1111' WHERE S_NATIONKEY = 23

%%sql
select * from zzDelta_SUPPLIER_part1  WHERE S_NATIONKEY = 23


---Fabric
spark.conf.set("spark.sql.parquet.vorder.enabled", "true")
spark.conf.set("spark.microsoft.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.microsoft.delta.optimizeWrite.binSize", "1073741824")
src = "Files/copieddata/LINEITEM"
dst = "Tables/LINEITEM"
df = spark.read.parquet(src)
df.write.mode("overwrite").format("delta").save(dst)

